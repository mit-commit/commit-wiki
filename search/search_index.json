{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Commit Wiki Commit Mailing Lists & Talks Commit mailing list: Link Commit group meeting talks: Link . Commit group meeting talk/waci slides: Link . Upcoming Paper Deadlines: Link . Commit sysadmin mailing list (lanka/website/slack admins): commit-sysadmin@lists.csail.mit.edu Use the sysadmin mailing list to: Request to be added on the Commit Slack channel ( mitcommit.slack.com ). Request access to slurm on Lanka and to the dgx0 multi-GPU machine. Guides & HowTos Guide for using Lanka: Link . Guide for adding your papers to the commit website: Link . Guide for commit-admins: Link .","title":"Home"},{"location":"#commit-wiki","text":"","title":"Commit Wiki"},{"location":"#commit-mailing-lists-talks","text":"Commit mailing list: Link Commit group meeting talks: Link . Commit group meeting talk/waci slides: Link . Upcoming Paper Deadlines: Link . Commit sysadmin mailing list (lanka/website/slack admins): commit-sysadmin@lists.csail.mit.edu Use the sysadmin mailing list to: Request to be added on the Commit Slack channel ( mitcommit.slack.com ). Request access to slurm on Lanka and to the dgx0 multi-GPU machine.","title":"Commit Mailing Lists &amp; Talks"},{"location":"#guides-howtos","text":"Guide for using Lanka: Link . Guide for adding your papers to the commit website: Link . Guide for commit-admins: Link .","title":"Guides &amp; HowTos"},{"location":"admin/","text":"Commit Admins Guide Add a User to dgx0 CSAIL users cannot access dgx0 directly. A new dgx0 username has to be created separately. To do so: sudo adduser <username> When asked for a kerberos password, leave it empty (so that the user can use their original kerberos password). The uid/gid for the newly created user needs to be matched with their CSAIL kerberos uid/gid for the user to be able to access their files on the NFS. First, obtain the kerberos uid for the user by running the command on the lanka login node (not the dgx0): id <username> Now, assign the same uid/gid to the newly created user on the dgx0 with the commands: sudo usermod -u <id> <username> sudo usermod -g <id> <username> Verify the user has the correct uid/gid by running the command on the dgx0: id <username> Enable a User to Use Slurm on Lanka To enable a user to use Slurm: sudo sacctmgr add user name=<username> account=root","title":"Admin"},{"location":"admin/#commit-admins-guide","text":"","title":"Commit Admins Guide"},{"location":"admin/#add-a-user-to-dgx0","text":"CSAIL users cannot access dgx0 directly. A new dgx0 username has to be created separately. To do so: sudo adduser <username> When asked for a kerberos password, leave it empty (so that the user can use their original kerberos password). The uid/gid for the newly created user needs to be matched with their CSAIL kerberos uid/gid for the user to be able to access their files on the NFS. First, obtain the kerberos uid for the user by running the command on the lanka login node (not the dgx0): id <username> Now, assign the same uid/gid to the newly created user on the dgx0 with the commands: sudo usermod -u <id> <username> sudo usermod -g <id> <username> Verify the user has the correct uid/gid by running the command on the dgx0: id <username>","title":"Add a User to dgx0"},{"location":"admin/#enable-a-user-to-use-slurm-on-lanka","text":"To enable a user to use Slurm: sudo sacctmgr add user name=<username> account=root","title":"Enable a User to Use Slurm on Lanka"},{"location":"lanka/","text":"Using the Lanka Cluster Lanka is a 24-node Intel Xeon E5-2695 v2 @ 2.40GHz Infiniband cluster with two sockets per node, each with 12 cores, for a total of 576 cores and a theoretical peak computational rate of 11059 GFlop/s. Each node has 128GB of memory, of which ~120GB is safely usable. To access the frontend node, ssh <csail_username>@login.csail.mit.edu , then ssh slurm-login . DO NOT ssh into any of the compute nodes. Important : The cluster's compute nodes do not have access to afs. Therefore, you must create a directory in CSAIL's shared scratch file system corresponding to your username (i.e. /data/scratch/<username> ) and do all cluster computing from that directory. Email commit-sysadmin@lists.csail.mit.edu for an account. Running on Lanka with SLURM We use the SLURM workload manager for the batch system. If you're familiar with another batch system, the rosetta stone of workload managers may be helpful. Required SLURM Flags: --partition : Partitions available now are lanka-v2 and lanka-v3 --qos : the only acceptable value for us is commit-main . --time : in minutes. An example command would look like this srun -N 1 -n 1 --qos commit-main --time=60 --mem 102400 --partition lanka-v3 --exclusive ls Important commands sbatch <file> : submit <file> to the batch system (see below). scancel <job_id> : stop a job/delete it from the queue squeue : see queued jobs sinfo -N -l : view cluster status Running interactively For debugging or testing, you can run interactively on the cluster. This is done with the srun command. For example, to run a interactively for 1 hour, you would run srun --partition=lanka-v2 --qos=commit-main --time=01:00:00 --pty bash -i . Running batch jobs For most cases, the batch system is the way to go. Basically, write a simple batch script and submit it to the system via sbatch <foo.batch> and it will run when resources are available. For a full description of the sbatch syntax, see the man page or the SLURM website, but a quick example is below: #!/bin/bash #SBATCH --partition lanka-v3 #SBATCH --qos commit-main #SBATCH --tasks-per-node=24 #SBATCH -N 2 #SBATCH --cpu_bind=verbose,cores #SBATCH --exclusive #SBATCH -t 10 cd $SLURM_SUBMIT_DIR srun --cpu_bind=verbose,cores ./hellompi Compiling for Lanka Currently, MVAPICH2 is installed as the MPI implementation on the cluster. MPI compilers are located in /usr/local/bin and you may need to ensure /usr/local/lib appears in your LD_LIBRARY_PATH . Building on csail login machines is discouraged. Instead, build on a lanka compute node. Note that the different kinds of lanka nodes are different architectures, so be careful about building on e.g. lanka-v2 and running on lanka-v3. spack may help for cross-compilation. Turbo Boost Turbo Boost has been disabled on all nodes to reduce timing inconsistencies and performance variability. This setting should remain off by default. Check with cat /sys/devices/system/cpu/intel_pstate/no_turbo (should return 1). You can fail if turbo boost is enabled with [ \"$(cat /sys/devices/system/cpu/intel_pstate/no_turbo)\" = \"1\" ] || (echo \"TurboBoost is on and could disrupt benchmarks. Failing...\" && exit 1) . File Permissions and .bashrc Since the compute nodes do not have direct access to AFS, where your .bashrc file is typically located, a common issue arises in maintaining environment settings across jobs. A practical solution is to utilize a soft link to your .bashrc within a directory inside AFS, configured with specific permissions to allow system-wide readability without the need for Kerberos tokens. Steps to Setup the Soft Link for .bashrc Create a Publicly Readable Directory in AFS : First, you need to create a directory within your afs directory that will store your .bashrc . This directory must be set with permissions that allow system:anyuser to read and list contents. For example, if you're creating a directory named public_configs in your AFS space, you would use the following commands: mkdir ~/public_configs fs setacl ~/public_configs system:anyuser rl Move .bashrc to the Public Directory : Move your .bashrc file to this newly created directory. This ensures that the file is accessible from compute nodes within the Lanka Cluster. mv ~/.bashrc ~/public_configs/ Create a Soft Link in Your Home Directory : To ensure seamless integration with your environment, create a symbolic link in your home directory that points to the .bashrc file in the public_configs directory. ln -s ~/public_configs/.bashrc ~/.bashrc Ensuring SLURM Jobs Source Your .bashrc Now you may run your bashrc explicitly from a SLURM job: source ~/public_configs/.bashrc Other Machines Non-Slurm Machine Etiquette Since these machines are shared, you need to coordinate with other users to avoid affecting other people's results! Please post in the lanka slack channel. Do not let others use these machines without making them do this! The Lanka-dgx0 cluster. The dgx0 machine has eight Tesla V100-SXM2-32GB GPUs. You can access the dgx0 machine via ssh lanka-dgx0 from within csail. We don't use SLURM on this machine so please try to coordinate with others in the lanka slack channel and least check via who . To configure which GPU you use, set CUDA_VISIBLE_DEVICES to the desired GPU(s) (0-7). For more details on the machine, specifically the cuda drivers, check nvidia-smi . This machine is currently running ubuntu 20 (compared to 22) on the other machines. The bigram cluster. The bigram machine is a Intel(R) Xeon(R) Platinum 8180 CPU cluster (with 224 processors) and with roughly three terrabytes of RAM. You can access the bigram machine via ssh bigram from within csail (e.g. from the login node). We don't use SLURM on this machine so please try to coordinate with others in the lanka slack channel and check via who . Use the taskset command to configure which CPUs a given task uses.","title":"Lanka"},{"location":"lanka/#using-the-lanka-cluster","text":"Lanka is a 24-node Intel Xeon E5-2695 v2 @ 2.40GHz Infiniband cluster with two sockets per node, each with 12 cores, for a total of 576 cores and a theoretical peak computational rate of 11059 GFlop/s. Each node has 128GB of memory, of which ~120GB is safely usable. To access the frontend node, ssh <csail_username>@login.csail.mit.edu , then ssh slurm-login . DO NOT ssh into any of the compute nodes. Important : The cluster's compute nodes do not have access to afs. Therefore, you must create a directory in CSAIL's shared scratch file system corresponding to your username (i.e. /data/scratch/<username> ) and do all cluster computing from that directory. Email commit-sysadmin@lists.csail.mit.edu for an account.","title":"Using the Lanka Cluster"},{"location":"lanka/#running-on-lanka-with-slurm","text":"We use the SLURM workload manager for the batch system. If you're familiar with another batch system, the rosetta stone of workload managers may be helpful.","title":"Running on Lanka with SLURM"},{"location":"lanka/#required-slurm-flags","text":"--partition : Partitions available now are lanka-v2 and lanka-v3 --qos : the only acceptable value for us is commit-main . --time : in minutes. An example command would look like this srun -N 1 -n 1 --qos commit-main --time=60 --mem 102400 --partition lanka-v3 --exclusive ls","title":"Required SLURM Flags:"},{"location":"lanka/#important-commands","text":"sbatch <file> : submit <file> to the batch system (see below). scancel <job_id> : stop a job/delete it from the queue squeue : see queued jobs sinfo -N -l : view cluster status","title":"Important commands"},{"location":"lanka/#running-interactively","text":"For debugging or testing, you can run interactively on the cluster. This is done with the srun command. For example, to run a interactively for 1 hour, you would run srun --partition=lanka-v2 --qos=commit-main --time=01:00:00 --pty bash -i .","title":"Running interactively"},{"location":"lanka/#running-batch-jobs","text":"For most cases, the batch system is the way to go. Basically, write a simple batch script and submit it to the system via sbatch <foo.batch> and it will run when resources are available. For a full description of the sbatch syntax, see the man page or the SLURM website, but a quick example is below: #!/bin/bash #SBATCH --partition lanka-v3 #SBATCH --qos commit-main #SBATCH --tasks-per-node=24 #SBATCH -N 2 #SBATCH --cpu_bind=verbose,cores #SBATCH --exclusive #SBATCH -t 10 cd $SLURM_SUBMIT_DIR srun --cpu_bind=verbose,cores ./hellompi","title":"Running batch jobs"},{"location":"lanka/#compiling-for-lanka","text":"Currently, MVAPICH2 is installed as the MPI implementation on the cluster. MPI compilers are located in /usr/local/bin and you may need to ensure /usr/local/lib appears in your LD_LIBRARY_PATH . Building on csail login machines is discouraged. Instead, build on a lanka compute node. Note that the different kinds of lanka nodes are different architectures, so be careful about building on e.g. lanka-v2 and running on lanka-v3. spack may help for cross-compilation.","title":"Compiling for Lanka"},{"location":"lanka/#turbo-boost","text":"Turbo Boost has been disabled on all nodes to reduce timing inconsistencies and performance variability. This setting should remain off by default. Check with cat /sys/devices/system/cpu/intel_pstate/no_turbo (should return 1). You can fail if turbo boost is enabled with [ \"$(cat /sys/devices/system/cpu/intel_pstate/no_turbo)\" = \"1\" ] || (echo \"TurboBoost is on and could disrupt benchmarks. Failing...\" && exit 1) .","title":"Turbo Boost"},{"location":"lanka/#file-permissions-and-bashrc","text":"Since the compute nodes do not have direct access to AFS, where your .bashrc file is typically located, a common issue arises in maintaining environment settings across jobs. A practical solution is to utilize a soft link to your .bashrc within a directory inside AFS, configured with specific permissions to allow system-wide readability without the need for Kerberos tokens.","title":"File Permissions and .bashrc"},{"location":"lanka/#steps-to-setup-the-soft-link-for-bashrc","text":"Create a Publicly Readable Directory in AFS : First, you need to create a directory within your afs directory that will store your .bashrc . This directory must be set with permissions that allow system:anyuser to read and list contents. For example, if you're creating a directory named public_configs in your AFS space, you would use the following commands: mkdir ~/public_configs fs setacl ~/public_configs system:anyuser rl Move .bashrc to the Public Directory : Move your .bashrc file to this newly created directory. This ensures that the file is accessible from compute nodes within the Lanka Cluster. mv ~/.bashrc ~/public_configs/ Create a Soft Link in Your Home Directory : To ensure seamless integration with your environment, create a symbolic link in your home directory that points to the .bashrc file in the public_configs directory. ln -s ~/public_configs/.bashrc ~/.bashrc","title":"Steps to Setup the Soft Link for .bashrc"},{"location":"lanka/#ensuring-slurm-jobs-source-your-bashrc","text":"Now you may run your bashrc explicitly from a SLURM job: source ~/public_configs/.bashrc","title":"Ensuring SLURM Jobs Source Your .bashrc"},{"location":"lanka/#other-machines","text":"","title":"Other Machines"},{"location":"lanka/#non-slurm-machine-etiquette","text":"Since these machines are shared, you need to coordinate with other users to avoid affecting other people's results! Please post in the lanka slack channel. Do not let others use these machines without making them do this!","title":"Non-Slurm Machine Etiquette"},{"location":"lanka/#the-lanka-dgx0-cluster","text":"The dgx0 machine has eight Tesla V100-SXM2-32GB GPUs. You can access the dgx0 machine via ssh lanka-dgx0 from within csail. We don't use SLURM on this machine so please try to coordinate with others in the lanka slack channel and least check via who . To configure which GPU you use, set CUDA_VISIBLE_DEVICES to the desired GPU(s) (0-7). For more details on the machine, specifically the cuda drivers, check nvidia-smi . This machine is currently running ubuntu 20 (compared to 22) on the other machines.","title":"The Lanka-dgx0 cluster."},{"location":"lanka/#the-bigram-cluster","text":"The bigram machine is a Intel(R) Xeon(R) Platinum 8180 CPU cluster (with 224 processors) and with roughly three terrabytes of RAM. You can access the bigram machine via ssh bigram from within csail (e.g. from the login node). We don't use SLURM on this machine so please try to coordinate with others in the lanka slack channel and check via who . Use the taskset command to configure which CPUs a given task uses.","title":"The bigram cluster."},{"location":"papers/","text":"How to Add a Paper to the Commit Website? We use Git to manage updates to the whole commit website. DO NOT EDIT FILES IN /afs/csail.mit.edu/group/commit/www/data (we used to use a different system for this, but we are now using Git). To add a paper: - Fork the commit website at https://github.com/mit-commit/commit-website . git clone your fork. Create a new branch for your changes. Add the PDF file of your paper to /data/papers/ . Make sure you add it in the right year (for example, papers of 2020 should be in the folder /data/papers/2020/ ). If the folder does not exist, feel free to create it. Add a BibTeX entry for the paper in papers.bib . Make sure you add a link to your paper, for example using URL. For example: url=\"http://groups.csail.mit.edu/commit/papers/2018/tiramisu_paper.pdf\" DO NOT run add_papers.sh like before, an admin will do it for you after merging your pull request Commit your changes to git, push them to your fork repository, and create a pull request on the main repository. You are done. After merging your PR, an admin will update the server based on the master branch of the git repo. Please keep in mind that the script used to extract information from the file papers.bib expects the BibTeX to be in a certain format. For example, the BibTeX entries have to be between \" \" . This is why it might be a good idea to copy another similar entry and modify it. If there any issues compiling your added bibtex entry, an admin will contact you before merging. Please look out for mails/comments on the PR to help them fix the bug in a timely manner. Consider sending an email to the commit mailing list to let everyone know about the new paper.","title":"Papers"},{"location":"papers/#how-to-add-a-paper-to-the-commit-website","text":"We use Git to manage updates to the whole commit website. DO NOT EDIT FILES IN /afs/csail.mit.edu/group/commit/www/data (we used to use a different system for this, but we are now using Git). To add a paper: - Fork the commit website at https://github.com/mit-commit/commit-website . git clone your fork. Create a new branch for your changes. Add the PDF file of your paper to /data/papers/ . Make sure you add it in the right year (for example, papers of 2020 should be in the folder /data/papers/2020/ ). If the folder does not exist, feel free to create it. Add a BibTeX entry for the paper in papers.bib . Make sure you add a link to your paper, for example using URL. For example: url=\"http://groups.csail.mit.edu/commit/papers/2018/tiramisu_paper.pdf\" DO NOT run add_papers.sh like before, an admin will do it for you after merging your pull request Commit your changes to git, push them to your fork repository, and create a pull request on the main repository. You are done. After merging your PR, an admin will update the server based on the master branch of the git repo. Please keep in mind that the script used to extract information from the file papers.bib expects the BibTeX to be in a certain format. For example, the BibTeX entries have to be between \" \" . This is why it might be a good idea to copy another similar entry and modify it. If there any issues compiling your added bibtex entry, an admin will contact you before merging. Please look out for mails/comments on the PR to help them fix the bug in a timely manner. Consider sending an email to the commit mailing list to let everyone know about the new paper.","title":"How to Add a Paper to the Commit Website?"},{"location":"zotero/","text":"Commit Group Zotero The Commit Group Zotero is here: https://www.zotero.org/groups/4421826 You can use it to collaborate on papers and contribute those citations to everyone. Bug someone with admin access to be added!","title":"Zotero"},{"location":"zotero/#commit-group-zotero","text":"The Commit Group Zotero is here: https://www.zotero.org/groups/4421826 You can use it to collaborate on papers and contribute those citations to everyone. Bug someone with admin access to be added!","title":"Commit Group Zotero"}]}